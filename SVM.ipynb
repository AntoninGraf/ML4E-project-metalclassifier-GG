{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize coin data to spot specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import h5py\n",
    "from util.settings import Dtype, Settings\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from datafileviewer_template import DataFileViewer\n",
    "from datafilereader import DataFileReader\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./data/Groupe5/dataSetAGF-bobine3/\"\n",
    "\n",
    "coinNames = [\"5_CTS\", \"10_CTS\", \"20_CTS\", \"50_CTS\", \"1_CHF\", \"2_CHF\", \"5_CHF\"]\n",
    "\n",
    "#get frequecy at the first coin (always the same)\n",
    "dataset = DataFileReader(folder+coinNames[0]+\".h5\")\n",
    "f,_ = dataset.get_all_mesurements()\n",
    "\n",
    "# get Z for all coins\n",
    "coins_Z = []\n",
    "for i in range(1, 8):\n",
    "    dataset = DataFileReader(folder+coinNames[i-1]+\".h5\")\n",
    "    _,Z = dataset.get_all_mesurements()\n",
    "    coins_Z.append([Z])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute mean for each coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins_mean = []\n",
    "\n",
    "for i in range(len(coins_Z)):\n",
    "    N = len(coins_Z[i][0]) #number of measurements for this coin\n",
    "    R = np.real(coins_Z[i][0])\n",
    "    L = np.imag(coins_Z[i][0])/(2*np.pi*f)\n",
    "    # substract all the data by the calibration\n",
    "    R = R[1:,:]-R[0,:]\n",
    "    L = L[1:,:]-L[0,:]\n",
    "    # calculate the mean\n",
    "    R_mean = np.mean(R, axis=0)\n",
    "    L_mean = np.mean(L, axis=0)\n",
    "    coins_mean.append([R_mean,L_mean])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search optimal features (frequency) for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featureListR = []\n",
    "featureListL = []\n",
    "#search best frequencies for R\n",
    "for i in range(len(coins_Z)):\n",
    "    for j in range(i+1,len(coins_Z)):\n",
    "        dif_R  = np.abs(coins_mean[i][0] - coins_mean[j][0])\n",
    "        idxMax = np.argmax(dif_R)\n",
    "        if idxMax not in featureListR:\n",
    "            featureListR.append(idxMax)\n",
    "\n",
    "#search best frequencies for L\n",
    "for i in range(len(coins_Z)):\n",
    "    for j in range(i+1,len(coins_Z)):\n",
    "        dif_L  = np.abs(coins_mean[i][1] - coins_mean[j][1])\n",
    "        idxMax = np.argmax(dif_L)\n",
    "        if idxMax not in featureListL:\n",
    "            featureListL.append(idxMax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all data and start SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and merge data from all groups for each coin, using the optimal features\n",
    "def load_and_merge_data_with_features(groups, coins, data_dir, feature_indices_R, feature_indices_L):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for coin in coins:\n",
    "        coin_name = coin.replace('.h5', '')\n",
    "        for group in groups:\n",
    "            file_path = os.path.join(data_dir, group, coin)\n",
    "            if os.path.isfile(file_path):\n",
    "                reader = DataFileReader(file_path)\n",
    "                frequency, Z = reader.get_all_mesurements()\n",
    "                \n",
    "                # Separate resistance and reactance\n",
    "                resistance = np.real(Z)\n",
    "                reactance = np.imag(Z) / (2 * np.pi * frequency)\n",
    "                \n",
    "                # Extract features based on optimal frequencies\n",
    "                for idx in range(resistance.shape[0]):\n",
    "                    features = [\n",
    "                        resistance[idx, feature_indices_R],\n",
    "                        reactance[idx, feature_indices_L]\n",
    "                    ]\n",
    "                    features = np.concatenate(features)\n",
    "                    data.append(features)\n",
    "                    labels.append(coin_name)\n",
    "            \n",
    "    data, labels = shuffle(np.array(data), np.array(labels))\n",
    "    return pd.DataFrame(data), pd.Series(labels)\n",
    "\n",
    "# Define the directory and groups\n",
    "data_dir = \"./data\"\n",
    "num_groups = 11\n",
    "groups = [f\"Groupe{i}\" for i in range(1, num_groups + 1)]\n",
    "coins = [\"1_CHF.h5\", \"2_CHF.h5\", \"5_CHF.h5\", \"10_CTS.h5\", \"20_CTS.h5\", \"50_CTS.h5\", \"EUR_1.h5\"]\n",
    "\n",
    "# Load and merge data using the identified optimal features\n",
    "feature_indices_R = sorted(featureListR)  # Use the optimal features for resistance\n",
    "feature_indices_L = sorted(featureListL)  # Use the optimal features for reactance\n",
    "\n",
    "X, y = load_and_merge_data_with_features(groups, coins, data_dir, feature_indices_R, feature_indices_L)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      10_CTS       0.74      0.63      0.68        27\n",
      "       1_CHF       0.65      0.91      0.76        33\n",
      "      20_CTS       1.00      0.60      0.75        30\n",
      "       2_CHF       1.00      0.95      0.97        39\n",
      "      50_CTS       0.93      1.00      0.96        27\n",
      "       5_CHF       0.93      1.00      0.96        41\n",
      "\n",
      "    accuracy                           0.86       197\n",
      "   macro avg       0.88      0.85      0.85       197\n",
      "weighted avg       0.88      0.86      0.86       197\n",
      "\n",
      "[[17  8  0  0  2  0]\n",
      " [ 2 30  0  0  0  1]\n",
      " [ 4  8 18  0  0  0]\n",
      " [ 0  0  0 37  0  2]\n",
      " [ 0  0  0  0 27  0]\n",
      " [ 0  0  0  0  0 41]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the SVM model with a non-linear kernel\n",
    "svm_model = SVC(kernel='rbf', probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "print(\"Validation Set Report\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(confusion_matrix(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      10_CTS       0.69      0.55      0.61        20\n",
      "       1_CHF       0.67      0.71      0.69        31\n",
      "      20_CTS       0.87      0.77      0.82        35\n",
      "       2_CHF       1.00      0.93      0.97        30\n",
      "      50_CTS       0.90      0.96      0.93        27\n",
      "       5_CHF       0.90      1.00      0.95        54\n",
      "\n",
      "    accuracy                           0.85       197\n",
      "   macro avg       0.84      0.82      0.83       197\n",
      "weighted avg       0.85      0.85      0.85       197\n",
      "\n",
      "[[11  5  1  0  3  0]\n",
      " [ 4 22  3  0  0  2]\n",
      " [ 1  6 27  0  0  1]\n",
      " [ 0  0  0 28  0  2]\n",
      " [ 0  0  0  0 26  1]\n",
      " [ 0  0  0  0  0 54]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "print(\"Test Set Report\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(confusion_matrix(y_test, y_test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
